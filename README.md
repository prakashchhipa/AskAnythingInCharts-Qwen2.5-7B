# AskAnything in Charts - Powered by Qwen 2.5

LORA Fine-tuned Qwen 2.5 7B vision-language model with the base model for chart understanding tasks. Finetuned adapted for pinpoint answer for question on ChartQA benchmark, acheiving 8% improvmeent. The model is trained on the ChartQA dataset to better understand and answer questions about charts and graphs.

**Author:** [Prakash Chandra Chhipa](https://github.com/prakashchhipa)  
**Portfolio:** [prakashchhipa.github.io](https://prakashchhipa.github.io)  
**GitHub:** [AskAnythingInCharts-Qwen2.5-7B](https://github.com/prakashchhipa/AskAnythingInCharts-Qwen2.5-7B)

## ğŸš€ Try It Live

- **ğŸ¨ Interactive Demo:** [HuggingFace Spaces](https://huggingface.co/spaces/prakashchhipa/chart-qa-demo-qwen2.5)
- **ğŸ¤— Model Card:** [HuggingFace Hub](https://huggingface.co/prakashchhipa/Qwen2.5-VL-7B-ChartQA-LoRA)

## ğŸ¬ Demo

### Animated Demo
![Animated Demo](demo_animation.gif)

<p align="center">
  <img src="https://img.shields.io/badge/Model-Qwen2.5--VL--7B-blue" alt="Model">
  <img src="https://img.shields.io/badge/Dataset-ChartQA-green" alt="Dataset">
  <img src="https://img.shields.io/badge/Accuracy-66.0%25-brightgreen" alt="Accuracy">
  <img src="https://img.shields.io/badge/Framework-HuggingFace-orange" alt="Framework">
</p>

<p align="center">
  <a href="https://huggingface.co/spaces/prakashchhipa/chart-qa-demo-qwen2.5">
    <img src="https://img.shields.io/badge/ğŸ¨_Try_Demo-HuggingFace_Spaces-blue?style=for-the-badge" alt="Try Demo">
  </a>
  <a href="https://huggingface.co/prakashchhipa/Qwen2.5-VL-7B-ChartQA-LoRA">
    <img src="https://img.shields.io/badge/ğŸ¤—_Model-HuggingFace_Hub-green?style=for-the-badge" alt="Model Card">
  </a>
</p>

---

## ğŸ¯ Performance Comparison

| Model | ChartQA Accuracy | Improvement |
|-------|------------------|-------------|
| Qwen 2.5 7B | **57.5%** | - |
| **Qwen 2.5 7B + LORA SFT** | **66.0%** | **+8.5%** |

---

---

## ğŸš€ Try It Yourself

### Option 1: Online Demo (HuggingFace Spaces)

**Coming Soon:** [Your HF Space URL]

### Option 2: Run Locally

```bash
# 1. Clone the repository
git clone [your-repo-url]
cd openvision-assistant

# 2. Install dependencies
pip install -r requirements_demo.txt

# 3. Download the fine-tuned adapter (if not included)
# Make sure outputs/qwen2_5_vl_7b_lora_rank64_e6/ exists

# 4. Run the Gradio demo
python app.py
```

Open your browser at `http://localhost:7860`

### Option 3: Use the Model Directly

```python
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from peft import PeftModel
from PIL import Image

# Load model
base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-7B-Instruct",
    torch_dtype="bfloat16",
    device_map="auto"
)
model = PeftModel.from_pretrained(base_model, "prakashchhipa/Qwen2.5-VL-7B-ChartQA-LoRA")
model = model.merge_and_unload()

processor = AutoProcessor.from_pretrained("prakashchhipa/Qwen2.5-VL-7B-ChartQA-LoRA")

# Inference
image = Image.open("chart.png")
question = "What is the highest value in the chart?"

messages = [
    {"role": "user", "content": [
        {"type": "text", "text": question},
        {"type": "image", "image": image}
    ]}
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
# ... (process vision info and generate)
```

---

## ğŸ“– What's Improved?

The fine-tuned model shows better performance in:

| Category | Description |
|----------|-------------|
| âœ… **Concise Answers** | Returns exact values without verbose explanations |
| âœ… **Label Recognition** | Better at reading text labels from charts |
| âœ… **Color Identification** | More accurate at identifying chart colors |
| âœ… **Statistical Calculations** | Improved at medians, ratios, differences |
| âœ… **Counting** | Better accuracy in counting chart elements |
| âœ… **Region Comparison** | Accurate comparisons across chart regions |
| âœ… **Yes/No Questions** | More reliable binary responses |

---

## ğŸ› ï¸ Technical Details

### Model Architecture
- **Base Model**: [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) (7B parameters)
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
  - Rank: 64
  - Alpha: 16
  - Target modules: Vision and language attention layers

### Training Setup
- **Dataset**: ChartQA (chart understanding benchmark)
- **Training Samples**: ~50-200 samples per epoch
- **Epochs**: 6
- **Learning Rate**: 4e-5
- **LoRA Rank**: 64
- **LoRA Alpha**: 16
- **Hardware**: GPU with 16GB+ VRAM
- **Framework**: HuggingFace Transformers + PEFT + DeepSpeed

### Evaluation
- **Test Set**: ChartQA validation set (500 examples)
- **Metric**: Exact Match (with normalization and numeric tolerance)
- **Filtering**: Only genuine improvements (excluded verbose-but-correct cases)

---

## ğŸ“ Repository Structure

```
openvision-assistant/
â”œâ”€â”€ app.py                              # Gradio interactive demo
â”œâ”€â”€ requirements_demo.txt               # Demo dependencies
â”œâ”€â”€ find_improved_examples.py           # Find improved examples
â”œâ”€â”€ filter_genuine_improvements.py      # Filter genuine improvements
â”œâ”€â”€ demo_genuine/                       # Genuine improvement examples
â”‚   â”œâ”€â”€ results.json
â”‚   â”œâ”€â”€ example_*.png
â”‚   â””â”€â”€ genuine_improvements.html
â”œâ”€â”€ evaluations/
â”‚   â””â”€â”€ eval_chartqa.py                 # Evaluation script
â”œâ”€â”€ src/
â”‚   â””â”€â”€ train_vlm_sft.py                # Training script
â””â”€â”€ outputs/
    â””â”€â”€ qwen2_5_vl_7b_lora_rank64_e6/  # Fine-tuned weights
```

---

## ğŸ”¬ Reproduce Results

### 1. Evaluate on ChartQA

```bash
python evaluations/eval_chartqa.py \
  --base_model Qwen/Qwen2.5-VL-7B-Instruct \
  --adapter prakashchhipa/Qwen2.5-VL-7B-ChartQA-LoRA \
  --limit 500 \
  --compare_both
```

### 2. Find Improved Examples

```bash
python find_improved_examples.py \
  --base_model Qwen/Qwen2.5-VL-7B-Instruct \
  --adapter prakashchhipa/Qwen2.5-VL-7B-ChartQA-LoRA \
  --limit 500 \
  --output_dir demo_chatqa
```

### 3. Filter Genuine Improvements

```bash
python filter_genuine_improvements.py \
  --input_dir demo_chatqa/improved \
  --output_dir demo_genuine
```

---

## ğŸ“¦ Deploy to HuggingFace Spaces

### Step 1: Prepare Files

```bash
# Create a new directory for HF Space
mkdir chartqa-demo
cd chartqa-demo

# Copy necessary files
cp app.py requirements_demo.txt README_DEMO.md .
cp -r demo_genuine/ .

# Upload your adapter to HuggingFace Hub first, then update app.py:
# ADAPTER_PATH = "your-username/your-adapter-name"
```

### Step 2: Create HF Space

1. Go to [HuggingFace Spaces](https://huggingface.co/spaces)
2. Click "Create new Space"
3. Choose "Gradio" as SDK
4. Upload your files
5. Add `.env` file with model paths (if needed)

### Step 3: Space Configuration

Create `README.md` in your Space with:

```yaml
---
title: Chart Understanding with Fine-tuned Qwen2.5-VL
emoji: ğŸ“Š
colorFrom: blue
colorTo: green
sdk: gradio
sdk_version: 4.0.0
app_file: app.py
pinned: false
---
```

---

## ğŸ’¡ Use Cases

This model is useful for:

- ğŸ“ˆ **Business Analytics**: Extract insights from business charts
- ğŸ“Š **Data Analysis**: Answer questions about data visualizations
- ğŸ“‘ **Report Processing**: Understand charts in documents
- ğŸ”¬ **Research**: Analyze scientific plots and graphs
- ğŸ“± **Accessibility**: Make charts accessible through Q&A
- ğŸ¤– **Automation**: Automate chart data extraction

---

## âš™ï¸ Requirements

```txt
gradio>=4.0.0
torch>=2.0.0
transformers>=4.45.0
peft>=0.12.0
Pillow>=10.0.0
qwen-vl-utils>=0.0.8
accelerate>=0.20.0
```

**Hardware:**
- **Inference**: GPU with 16GB+ VRAM (or CPU with patience)
- **Training**: GPU with 24GB+ VRAM recommended

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- [ ] Add more chart types (scatter plots, heatmaps, etc.)
- [ ] Improve training data diversity
- [ ] Optimize inference speed
- [ ] Add multi-turn conversation support
- [ ] Create mobile-friendly interface

---

## ğŸ“„ License

This project is released under the MIT License. The base Qwen2.5-VL model is subject to its own license terms.

---

## ğŸ™ Acknowledgments

- **Base Model**: [Qwen Team](https://github.com/QwenLM/Qwen2-VL) for Qwen2.5-VL-7B
- **Dataset**: [ChartQA](https://github.com/vis-nlp/ChartQA) benchmark
- **Framework**: [HuggingFace](https://huggingface.co/) Transformers and PEFT
- **UI**: [Gradio](https://gradio.app/) for the interactive interface

---

## ğŸ“š Citation

If you use this model or code in your research, please cite:

```bibtex
@misc{chartqa-finetuned-qwen,
  title={Fine-tuned Qwen2.5-VL-7B for Chart Understanding},
  author={[Your Name]},
  year={2025},
  url={[Your Repo URL]}
}
```

---

## ğŸ“§ Contact

For questions or feedback:
- Open an issue on GitHub

---

<p align="center">
  <b>Built with â¤ï¸ using Qwen2.5-VL and HuggingFace Transformers</b>
</p>

